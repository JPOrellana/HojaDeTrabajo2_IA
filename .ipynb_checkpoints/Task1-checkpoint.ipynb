{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8cc43a2-2b6d-40dd-b78f-9d11e7ef9108",
   "metadata": {},
   "source": [
    "### Hoja de Trabajo 2\n",
    "-------------\n",
    "- Diego Alberto Leiva 21752\n",
    "- José Pablo Orellana 21970\n",
    "- Gabriel Estuardo García 21352\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4375e3ce-9550-4114-a649-0ebc9e7a232a",
   "metadata": {},
   "source": [
    "Responda a cada de las siguientes preguntas de forma clara y lo más completamente posible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e8bf78-644e-44eb-8815-45cfd5cb98a8",
   "metadata": {},
   "source": [
    "-------------\n",
    "##### Defina el proceso de decisión de Markov (MDP) y explique sus componentes.\n",
    "El Proceso de Decisión de Markov (MDP, por sus siglas en inglés) es un modelo matemático utilizado en teoría de control y aprendizaje por refuerzo para representar situaciones de toma de decisiones secuenciales bajo incertidumbre. Los MDP son especialmente útiles en problemas donde las acciones tomadas en un estado dado afectan no solo las recompensas inmediatas, sino también los estados futuros y las recompensas asociadas. Los componentes clave de un MDP son los siguientes:          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e720a0-5038-4dc1-9714-6ff54bbbff64",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "- **Un conjunto de estados S:**  Cada estado representa una situación concreta del problema a resolver.\n",
    "- **Un conjunto de acciones A:** Las acciones permiten transitar entre estados. Cada estado está asociado a un subconjunto de acciones que son las que se pueden realizar desde ese estado.\n",
    "- **Una función de probabilidad p:** La probabilidad pij(a) denota la probabilidad de transitar al estado j si aplicamos la acción a desde el estado i.\n",
    "- **Una función de coste c:** El coste ci(a) especifica el coste asociado a realizar la acción a desde el estado i. (Ruiz Valverde, 2022).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d68f98f-e567-4eb8-bfd6-64a991271eae",
   "metadata": {},
   "source": [
    "-------------\n",
    "##### ¿Cuáles son algunos desafíos o limitaciones comunes asociados con la resolución de MDP a gran escala? Discuta los enfoques potenciales para abordar estos desafíos.\n",
    "- **Explosión de estados (State Space Explosion):** A medida que el número de estados posibles aumenta, el espacio de estados puede volverse prohibitivamente grande. Esto hace que sea difícil almacenar y procesar toda la información necesaria para la toma de decisiones.\n",
    "    - **Enfoque:** Métodos de aproximación, como la aproximación de funciones de valor o la aproximación de políticas, pueden ayudar a manejar el espacio de estados grande.\n",
    "- **Explosión de acciones (Action Space Explosion):** Similar al problema de la explosión de estados, el número de acciones posibles puede volverse grande, especialmente en entornos continuos.\n",
    "    - **Enfoque:** Reducción de dimensionalidad mediante técnicas como la selección de características relevantes o el uso de técnicas específicas para entornos continuos.\n",
    "- **Complejidad computacional:** La resolución exacta de MDP a gran escala puede ser computacionalmente costosa y a menudo requiere mucho tiempo de procesamiento.\n",
    "    - **Enfoque:** Uso de métodos de aproximación, paralelización de cálculos, y técnicas de muestreo para estimar soluciones de manera eficiente.\n",
    "- **Modelo desconocido:** En algunos casos, el modelo de transición o las recompensas pueden ser desconocidos o difíciles de modelar con precisión.\n",
    "    - **Enfoque:** Enfoques basados en aprendizaje por refuerzo, donde el agente interactúa con el entorno para aprender directamente de la experiencia, pueden ser útiles.\n",
    "- **Exploración y explotación:** En entornos complejos, encontrar un equilibrio entre explorar nuevas acciones y explotar el conocimiento existente puede ser desafiante.\n",
    "    - **Enfoque:** Estrategias de exploración cuidadosamente diseñadas, como epsilon-greedy o métodos basados en la incertidumbre, pueden ayudar a abordar este problema.\n",
    "- **Horizonte temporal largo:** En problemas con horizontes temporales largos, es más difícil tomar decisiones óptimas debido a la incertidumbre acumulativa.\n",
    "    - **Enfoque:** Uso de métodos de aprendizaje por refuerzo con descuento para priorizar recompensas inmediatas, y técnicas de planificación con horizontes temporales recortados para hacer el problema manejable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a276cc4b-f118-4df5-96dc-d43461ec2a1c",
   "metadata": {},
   "source": [
    "-------------\n",
    "##### Describa cual es la diferencia entre política, evaluación de políticas, mejora de políticas e iteración de políticas en el contexto de los PDM.\n",
    "En el contexto de los Procesos de Decisión de Markov (MDP), los conceptos de política, evaluación de políticas, mejora de políticas e iteración de políticas son fundamentales para comprender cómo se abordan los problemas de toma de decisiones. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156f4c0c-1426-40ae-b47a-4076cf3474ed",
   "metadata": {},
   "source": [
    "- **Política (Policy):**\n",
    "    - Una política en un MDP es una estrategia que especifica la acción a tomar en cada estado del sistema. Puede ser determinista, donde se elige una acción específica en cada estado, o estocástica, donde se define una distribución de probabilidad sobre las acciones posibles.\n",
    "- **Evaluación de Políticas (Policy Evaluation):**\n",
    "    - Es el proceso de determinar la calidad o el valor de una política dada en un MDP. Implica calcular la función de valor de esa política, que mide la recompensa esperada a lo largo del tiempo al seguir esa política y puede expresarse mediante funciones de valor como la función de valor de estado o la función de valor de acción.\n",
    "- **Mejora de Políticas (Policy Improvement):**\n",
    "    - Este proceso implica modificar o actualizar una política con el objetivo de hacerla más efectiva, es decir, aumentar la recompensa esperada. La mejora de políticas se basa en la información obtenida durante la evaluación de políticas. Una política se considera mejor si para al menos un estado, la acción sugerida por la nueva política es preferible a la acción sugerida por la política anterior.\n",
    "- **Iteración de Políticas (Policy Iteration):**\n",
    "    - La iteración de políticas es un proceso iterativo que combina la evaluación y la mejora de políticas. Comienza con una política inicial, luego alterna entre la evaluación de esa política y la mejora de la misma. Este proceso continúa hasta que se alcanza una política óptima, es decir, una política que no se puede mejorar más."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59acda5a-0395-4379-9d35-f83be7f76be6",
   "metadata": {},
   "source": [
    "-------------\n",
    "##### Explique el concepto de factor de descuento (gamma) en los MDP. ¿Cómo influye en la toma de decisiones?\n",
    "- El factor de descuento, denotado como γ γ (gamma), es un parámetro fundamental en los Procesos de Decisión de Markov (MDP). Este factor influye en la toma de decisiones al modelar la preferencia del agente por recompensas inmediatas frente a recompensas futuras y al introducir la noción de  a importancia del tiempo en el proceso de toma de decisione El factor de descuento varía en el rango de 0 a 1, y su impacto en la toma de decisiones se puede entender de la siguiente manera.\n",
    "    - **γ = 0:** Cuando γ es igual a cero, el agente solo valora las recompensas inmediatas y no tiene en cuenta las recompensas futuras. Esto implica que el agente es \"miopemente\" enfocado en maximizar la recompensa inmediata y no considera las consecuencias a largo plazo de sus acciones.\n",
    "    - **γ = 1:** Cuando γ es igual a uno, el agente valora todas las recompensas, independientemente de cuándo ocurran en el tiempo. En este caso, el agente toma decisiones considerando tanto las recompensas inmediatas como las futuras, sin importar cuán distante esté en el futuro.\n",
    "    - **0 < γ < 1:** En la mayoría de los casos, el factor de descuento se elige en este rango. Un valor de γ cercano a 1 indica que el agente valora las recompensas futuras, pero da menos importancia relativa a medida que se van alejando en el tiempo. Esto refleja la noción de impaciencia del agente, que tiende a preferir recompensas inmediatas, pero aún tiene en cuenta las recompensas futuras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edffe7c-2df8-471e-99ab-910f8d174b76",
   "metadata": {},
   "source": [
    "-------------\n",
    "##### Analice la diferencia entre los algoritmos de iteración de valores y de iteración de políticas para resolver MDP.\n",
    "- La Iteración de Valores (Value Iteration) y la Iteración de Políticas (Policy Iteration) son dos enfoques fundamentales para resolver Procesos de Decisión de Markov (MDP). Ambos métodos están diseñados para encontrar la política óptima, es decir, la estrategia de toma de decisiones que maximiza la recompensa esperada a lo largo del tiempo.\n",
    "    - **Iteración de Valores:** En este enfoque, el algoritmo itera sobre las funciones de valor de los estados. Durante cada iteración, actualiza las estimaciones de los valores de los estados para converger hacia la función de valor óptima.\n",
    "    - **Iteración de Políticas:** Este método, en cambio, itera sobre las políticas directamente. Alternando entre evaluación y mejora de políticas, busca encontrar la política óptima al ajustar continuamente las decisiones tomadas en cada estado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab43c2d-79eb-463f-950e-35b8cd2ab979",
   "metadata": {},
   "source": [
    "-------------\n",
    "#### Referencias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc3f93a-a52c-4d41-b6a2-4190a47888f9",
   "metadata": {},
   "source": [
    "- García Hernández. (2009, diciembre). Acceleration of association‐rule based  markov decision processes. https://www.redalyc.org/pdf/474/47413020008.pdf\n",
    "- Ruiz Valverde, A. (2022). Algoritmos heurísticos para procesos de decisión de Markov (p. 69). Universidad de Málaga. https://riuma.uma.es/xmlui/bitstream/handle/10630/25975/Ruiz%20Valverde%20Antonio%20Memoria.pdf?sequence=1&isAllowed=y\n",
    "- Ruiz-Loza, S., & Hernandez, B. (2014). Markov Decision Process and Micro scenarios for Crowd Navigation and Collision Avoidance (spanish language). En Research in Computing Science (p. 116).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
